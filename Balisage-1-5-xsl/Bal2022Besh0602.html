<!DOCTYPE HTML><html lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Balisage: Adventures in Correcting XML Collation Problems with Python and XSLT</title>
      <link href="https://fonts.googleapis.com/css?family=PT+Sans+Narrow&amp;display=swap" rel="stylesheet">
      <meta name="viewport" content="width=device-width, initial-scale=1"><script type="text/JavaScript">
      var detailsElement = document.createElement("details");
      if (!("open" in detailsElement)) {
          document.write('<script src="..//js/bower_components/better-dom/dist/better-dom.js"><\/script>');
          document.write('<script src="..//js/bower_components/better-details-polyfill/dist/better-details-polyfill.js"><\/script>');
          document.write('<script src="..//js/classname.js"><\/script>');
      }
      /* Enable CSS styling of figure elements in IE:
       * https://xopus.com/devblog/2008/style-unknown-elements.html
       */
      var IEfix = document.createElement('figure');
    </script><style type="text/css" id="inverter" media="none">
      html {
        filter: invert(100%);
      }

      * {
        background-color: inherit;
      }

      /* do not invert SVG images */
      img:not([src*=".svg"]),
      [style*="url("] {
        filter: invert(100%);
      }
    </style>
      <link rel="stylesheet" href="balisage-plain.css" type="text/css">
      <meta name="keywords" content="collation, tokenization, normalization, alignment, Gothenburg model, Python, XSLT, stand-off markup, stand-off pointers">
      <link id="favicon" rel="shortcut icon" type="image/png" href="http://balisage.net/favicon.ico">
      <!--balisage-html.xsl--></head>
   <body>
      <div class="skipnav"><a href="#main">Skip to contents.</a></div>
      <div id="balisage-header" role="banner">
         <h1><!--* balisage-html.xsl 519.38 *--><i>Balisage:</i>&nbsp;<small>The Markup Conference</small></h1>
      </div>
      <div lang="en" class="article">
         <div class="titlepage">
            <h1 class="article-title" id="d2e5">Balisage Paper: Adventures in Correcting XML Collation Problems with Python and XSLT</h1>
            <h2 class="subtitle">Untangling the <span class="ital">Frankenstein Variorum</span></h2>
            <div class="author">
               <h3 class="author">Elisa E. Beshero-Bondar</h3>
               <div class="affiliation">
                  <p class="jobtitle">Professor of Digital Humanities</p>
                  <p class="jobtitle">Program Chair of Digital Media, Arts, and Technology</p>
                  <p class="orgname">Penn State Erie, The Behrend College</p>
               </div>
               <h5 class="author-email"><code class="email">&lt;<a class="email" href="mailto:eeb4@psu.edu">eeb4@psu.edu</a>&gt;</code></h5>
            </div>
            <div class="abstract">
               <h2>Abstract</h2>
               <p id="d2e12">The process of instructing a computer to compare texts, known as computer-aided collation,
                  might resemble trying to fix a power loom when the threads it is supposed to weave
                  together become tangled. The power of the automated weaving continues, with the threads
                  improperly aligned and the pattern broken in a way that can make it difficult to isolate
                  the cause of the problem. Automating a tedious process magnifies the complexity of
                  error-correction, sometimes calling for new tooling to help us perfect the weaving
                  or collating process.</p>
               <p id="d2e13">The authors are attempting to refine a collation algorithm to improve its alignment
                  of variant passages in the <a href="https://frankensteinvariorum.github.io/" class="link">Frankenstein Variorum</a> project. We have begun with a Python script that tokenizes and normalizes the texts
                  of the editions and delivers them to <a href="https://collatex.net/" class="link">collateX</a> for processing the collation and delivering TEI-conformant output for our project.
                  In post-processing stages after running the collation, we apply a series of XSLT transformations
                  to the collation output. This post-collation XSLT pipeline publishes the digital variorum
                  edition, which prepares each output witness in TEI XML to store information about
                  its own variance from the other editions. We have discussed that pipeline elsewhere,
                  but our interest in this paper is in efforts to repair and correct and improve the
                  collation process.</p>
               <p id="d2e21">We have applied Schematron and XSLT in post-processing to correct patterns of erroneous
                  alignments, but eventually realized that the problems we were trying to solve required
                  repairing the collation algorithm. We are now experimenting with revising the collation
                  algorithm in two ways: 1) by fine-tuning the text preparation algorithms we apply
                  in our Python file that delivers text to the collateX software, and 2) by attempting
                  to introduce those same text preparation algorithms entirely with XSLT using the Text
                  Alignment Network's XSLT application of <code class="code">tan:diff()</code> and <code class="code">tan:collate()</code>, introduced by Joel Kalvesmaki at the 2021 Balisage conference. In this paper we
                  discuss the challenges of figuring out where and how to intervene in the collation
                  process, and what we are learning about how far we can take XSLT and Schematron in
                  helping to automate the preparation, collation, and correction process.</p>
            </div>
            <hr>
         </div>
         <details class="toc">
            <summary>
               <h2 class="inline-heading">Table of Contents</h2>
            </summary>
            <dl>
               <dt><span class="section"><a href="#d2e63" class="toc">Cycles of time in the Gothenburg model for computer-aided collation</a></span></dt>
               <dt><span class="section"><a href="#d2e151" class="toc">The <span class="ital">Frankenstein Variorum</span> project and its production pipeline</a></span></dt>
               <dt><span class="section"><a href="#d2e440" class="toc">Snags in the Weaving</a></span></dt>
               <dd>
                  <dl>
                     <dt><span class="section"><a href="#d2e442" class="toc">A Schematron flashlight</a></span></dt>
                     <dt><span class="section"><a href="#d2e471" class="toc">XSLT for alignment correction</a></span></dt>
                     <dt><span class="section"><a href="#d2e555" class="toc">The <q>smashed-tokens</q> problem</a></span></dt>
                  </dl>
               </dd>
               <dt><span class="section"><a href="#d2e578" class="toc">XSLT for the win?</a></span></dt>
            </dl>
         </details>
         <div class="section" id="d2e63">
            <h2 class="title" style="clear: both">Cycles of time in the Gothenburg model for computer-aided collation</h2>
            <p id="d2e65">A project applying computer-aided collation, by which we mean the guidance of a computational
               system to compare texts, challenges our debugging capacities in ways that scale and
               consume far more time than humans may expect at the outset. Scholarly editors working
               on collation projects often do so in the context of preparing critical editions that
               present a text-bearing cultural object in all its variations. These editors tend to
               prioritize a high degree of accuracy in the marking of variants, and may come to a
               computer-aided collation project prepared for saintly patience and endurance to spot
               and correct each and every error. Yet endurance has its mortal limits, and computationally-generated
               errors require the developers' patient scrutiny to be directed not to the outputs
               but to the preparation of the inputs. Humanities scholars (the author included) may
               be tempted to hand-correct errors as they spot them to perfect an output for a swift
               project launch, but this is brittle and can even compound errors, even if applied
               by a computer script that finds and replaces every instance of a distinctive pattern
               in the output. We should limit correcting the output, even computationally, in favor
               of making changes at the guidance and preparation stages and re-running the collation
               software. Taking cycles of time to study the errors generated by a collation process
               should lead us to a state of enlightenment about precisely what we are comparing and
               precisely how it is to be compared. Do we consider computer collation a form of machine
               learning, in which we expend energy preparing the training set? But it is really the
               scholars who learn over cycles of collation re-processing, with enhanced awareness
               of their systems for comparison. Perhaps we should think of computer-aided collation
               as machine-assisted human learning.</p>
            <p id="d2e66">This paper marks a return to work on a digital collation project that began in the
               years just before the pandemic, and paused a moment in 2020-2021 when several of the
               original project members moved to new jobs and I, as the one leading the preparation
               of the collation data, concentrated on teaching and running a program in a new university
               during a time of global pandemic crisis disrupting university work. Returning to work
               on the <span class="ital">Frankenstein Variorum</span> has necessitated reorienting myself to complicated processes and reviewing the problems
               we identified as we last documented and left them. This has been a labor of reviewing
               and sampling our past code, updating its documentation, and trying to find new and
               better ways to continue our work. We set out to prepare a comparison of five distinct
               versions of the novel <span class="ital">Frankenstein</span>, without taking any single edition as the default standard. Rather, we seek to provide
               readers a reading view for each of the five versions that demonstrates which passages
               were altered and how heavily they were altered. The <span class="ital">Frankenstein Variorum</span> should help readers to see a heavily revised text and understand the abandoned, discontinuous
               branches of that revision.</p>
            <p id="d2e77">Much of this project has investigated methods for moving between hierarchical and
               <q>flat</q> text structures, moving back and forth between markup and strings as we thread our
               texts into the machine power-loom of collation software, examine the woven output,
               and contemplate how to untangle the snags and snarls we find in the <q>woven textile</q> of collation output. The Gothenburg model guides our work, an elaboration of five
               distinct stages of work necessary to guide machine-assisted collation as established
               in 2009 by the developers of collateX and Juxta in Gothenburg, Sweden: 
               
               
               <div class="orderedlist" id="d2e83">
                  <ol style="list-style-type: decimal;">
                     <li id="d2e84">
                        <p id="d2e85">Tokenization (deciding on the smallest units of comparison)</p>
                     </li>
                     <li id="d2e86">
                        <p id="d2e87">Normalization/Regularization (deciding what distinct features in the source documents
                           need to be suppressed from comparison. An easy example of a normlization step is indicating
                           that ampersands are stand in for the word <q>and</q>. A more difficult judgment call is the handling of notational metamarks or even editorial
                           markup that assist reading in one document, but should be skipped over by the collation
                           software as not meaningful for comparison with other documents.)</p>
                     </li>
                     <li id="d2e91">
                        <p id="d2e92">Alignment (locating via software parallel equivalent passages and their moments of
                           unison and divergence)</p>
                     </li>
                     <li id="d2e93">
                        <p id="d2e94">Analysis/Feedback (reviewing, correcting, adjusting the alignment algorithm as well
                           as normalization/regularization method</p>
                     </li>
                     <li id="d2e95">
                        <p id="d2e96">Visualization (finding an effective way to show and share the collation results)<sup class="fn-label"><a href="#d2e98" class="footnoteref" id="d2e98-ref">[1]</a></sup></p>
                     </li>
                  </ol>
               </div>
               </p>
            <p id="d2e112">These stages are usually numbered, but that numbering belies the cyclicality of the
               Gothenburg process. We scholars who collate have to try and try again to retool, rethink,
               re-run, and even this is not a steady process because we have to decide whether certain
               kinds of problems are best resolved by changing the <q>pre-processing</q> to improve the normalization prior to collation, or to apply corrections to collation
               output errors in <q>post-processing</q>. Collation projects of significant length and complexity require computational assistance
               to visualize not only the collation output but crucially to locate patterns in the
               collation errors. The Analysis/Feedback stage is critical and without giving it procedural
               care, we risk inaccuracies or the human error introduced during hand-correcting outputs.</p>
            <p id="d2e118">In returning to work on the <span class="ital">Frankenstein Variorum</span>, I have concentrated on the Analysis/Feedback stage to try to find patterns in the
               snags of our collation weave, and to try different normalization and alignment methods
               to improve the work. The work requires close observation of meticulous details (plenty
               of myopic eye fatigue), but also a return to the long view of our research goals.
               As David J. Birnbaum and Elena Spadini have discussed, normalization is interpretive,
               reflecting on transcription decisions and deciding on what properties should and should
               not be considered worthy of comparison.<sup class="fn-label"><a href="#d2e123" class="footnoteref" id="d2e123-ref">[2]</a></sup> In our project, the great challenge for the normalization process has been contending
               with the diplomatic markup of the Shelley-Godwin Archive's manuscript notebook edition
               of <span class="ital">Frankenstein</span>. The meticulous page-by-page markup structured on page surfaces and zones cannot
               be compared with the semantic trees representing the other print-based editions in
               the project, though deletions do matter, as do the insertion points of marginal additions
               and corrections. Alignment of our variant passages is improved by revisiting our normalization
               algorithms, and also by testing our software tools as soon as we recognize problems.</p>
            <p id="d2e138">In this paper, I venture that the software we choose to assist us with aligning collations
               matters not only for its effectiveness in applying an algorithm like Needleman Wunsch
               or Dekker or TAN Diff, but also for its capacity to guide the scholar in the documentation
               of a highly complex, time-consuming, cyclical process. In the following sections I
               first unfold the elaborate pipeline process of our collation and visualization, to
               then introduce problems we have found and efforts to resolve them by exploring a new
               method of alignment. XSLT has been vital to the pre-processing and post-processing
               of our edition and collation data, but now we find it may be a benefit for handling
               the entire process of tree flattening, string collation, and edition construction.
               Long ago in the early years of XSLT, John Bradley extolled its virtues not only for
               publishing documents but also for text scholarly analysis and research.<sup class="fn-label"><a href="#d2e140" class="footnoteref" id="d2e140-ref">[3]</a></sup> Though it is common in the Digital Humanities community now to think of Python or
               R Studio as the only necessary tools one needs for text analysis, the strengths of
               XSLT seem realized in its precision and elegance in transforming structured and unstructured
               text and its particular ease of documentation for the humanities researcher. In this
               paper, I contemplate a move from applying normalizing algorithms to <q>stringified XML</q> in Python to restating and revising those algorithms in XSLT.</p>
         </div>
         <div class="section" id="d2e151">
            <h2 class="title" style="clear: both">The <span class="ital">Frankenstein Variorum</span> project and its production pipeline</h2>
            <p id="d2e157">The <span class="ital">Frankenstein Variorum</span> project (hereafter referred to as <span class="ital">FV</span>) is an ongoing collation project that began during the recent 1818-2018 bicentennial
               celebrating the first publication of Mary Shelley's novel. We are constructing a digital
               variorum edition that highlights alterations to the novel <span class="ital">Frankenstein</span> over five key moments from its first drafting in 1816 to its author’s final revisions
               by 1831. If we think of each source edition for the collation as a <q>thread</q>, the collation algorithm can be said to weave five threads together into a woven
               pattern that helps us to identify the moments when the editions align together and
               where they diverge. The project applies automated collation, so far working with the
               software collateX to create a <q>weave</q> of five editions of the novel, output in TEI-conformant XML. We have shared papers
               at <span class="ital">Balisage</span> in previous years about our processing work, preparing differently-encoded source
               editions for machine-assisted collation,
               <sup class="fn-label"><a href="#d2e176" class="footnoteref" id="d2e176-ref">[4]</a></sup> and working with the output of collation to construct a <q>spine</q> file in TEI critical apparatus form, which coordinates the preparation of the edition
               files that hold elements locating moments of divergence from the other editions.<sup class="fn-label"><a href="#d2e190" class="footnoteref" id="d2e190-ref">[5]</a></sup></p>
            <p id="d2e216">Our source <q>threads</q> for weaving the collation pattern include two well-known digital editions: <a href="http://knarf.english.upenn.edu/" class="link">The Pennsylvania Electronic Edition (PAEE)</a>, an early hypertext edition produced at the University of Pennsylvania in the mid
               1990s by Stuart Curran and Jack Lynch that represents the 1818 and 1831 published
               editions of the novel, and the <a href="http://shelleygodwinarchive.org/contents/frankenstein/" class="link">Shelley-Godwin Archive's edition of the manuscript notebooks (S-GA)</a> published in 2013 by the University of Maryland. We prepared two other editions in
               XML representing William Godwin’s corrections of his daughter's edition from 1823,
               and the <q>Thomas copy</q>, which represents Mary Shelley’s corrections and proposed revisions written in a
               copy of the 1818 edition left in Italy with her friend Mrs. Thomas.</p>
            <p id="d2e228">Here is a summary of our project’s production pipeline, discussed in more detail in
               previous Balisage papers:
               
               <div class="orderedlist" id="d2e230">
                  <ol style="list-style-type: decimal;">
                     <li id="d2e231">
                        <p id="d2e232">Preparing differently-encoded XML files for collation. This involves several stages
                           of document analysis and pre-processing:
                           
                           <div class="itemizedlist" id="d2e234">
                              <ul>
                                 <li id="d2e235">
                                    <p id="d2e236">Identifying markup that indicates structures we care about: letter, chapter, paragraphs,
                                       and verse structures, for example. Where these change from version to version, we
                                       want to be able to track them by including this markup with the text of each edition
                                       in the collation.</p>
                                 </li>
                                 <li id="d2e237">
                                    <p id="d2e238">Re-sequencing margin annotations coded in the SGA edition of the manuscript notebook,
                                       as these margin annotations were encoded at the ends of each XML file and needed to
                                       be moved into reading order for collation. (For this resequencing, we wrote XSLT to
                                       follow the <code class="code">@xml:id</code>s and pointers on each SGA edition file).</p>
                                 </li>
                                 <li id="d2e242">
                                    <p id="d2e243">Determining and marking <q>chunk</q> boundaries (dividing the texts into units that mostly start and end the same way
                                       to facilitate comparison): We divided <span class="ital">Frankenstein</span> into 33 <q>chunks</q> roughly corresponding with chapter structures. (Later in refining the collation we
                                       subdivided some of these chunks (breaking some complicated chunks into 3 -5 <q>sub-chunks</q>) for a more granular comparison of shorter passages.)</p>
                                 </li>
                                 <li id="d2e254">
                                    <p id="d2e255"><q>Flattening</q> the edition files' structural markup into Trojan milestone elements (converting structural
                                       elements that wrap chapters, letters, and paragraphs like <code class="code">&lt;div xml:id="chap-ID"&gt;</code> into self-closed matched pairs of elements to mark the starts and ends of structures:
                                       <code class="code">div sID="chap-ID"/&gt;</code> and <code class="code">div eID="chap-ID"/&gt;</code>). The use of these empty milestones facililates the inclusion of markup in the collation
                                       when, for example, a version inserts a new paragraph of chapter boundary within an
                                       otherwise identical passage in the other editions. Ultimately this flattening in pre-processing
                                       facilitates post-processing of the edition files from text strings to XML holding
                                       collation data.
                                       <sup class="fn-label"><a href="#d2e264" class="footnoteref" id="d2e264-ref">[6]</a></sup>
                                       </p>
                                 </li>
                              </ul>
                           </div>
                           </p>
                     </li>
                     <li id="d2e286">
                        <p id="d2e287">Normalizing the text and processing the collation. Up to this point, we have been
                           applying a Python script to read the flattened XML as text, and to introduce a series
                           of about 25 normalizations via regular expression patterns that instruct collateX
                           to (among other things):
                           
                           <div class="itemizedlist" id="d2e289">
                              <ul>
                                 <li id="d2e290">
                                    <p id="d2e291">Convert <q>&amp;</q> into <q>and</q></p>
                                 </li>
                                 <li id="d2e296">
                                    <p id="d2e297">Ignore some angle-bracketed material that is not relevant to the collation: For example,
                                       convert <code class="code">&lt;surface.+?/&gt;</code> and <code class="code">&lt;zone.+?/&gt;</code> into <code class="code">""</code> (nothing) when comparing strings of the editions</p>
                                 </li>
                                 <li id="d2e305">
                                    <p id="d2e306">Simplify to ignore the attribute values on Trojan milestone elements. For example,
                                       remove the <code class="code">@sID</code> and <code class="code">@eID</code> attributes by converting <code class="code">(&lt;p)\s+.+?(/&gt;)</code> into <code class="code">$1$2</code> (or simply <code class="code">&lt;p/&gt;</code>) so that all paragraph markers are read as the same.</p>
                                 </li>
                              </ul>
                           </div>
                           
                           </p>
                     </li>
                     <li id="d2e319">
                        <p id="d2e320">Work with the output of the collation to prepare, in a series of stages, a very important
                           file that we call our <q>spine</q>, which contains the collation data that organizes the variorum edition project. The
                           format of the output is a prototype of a TEI critical apparatus, and in preliminary
                           stages it is not quite valid against the TEI. Here is an example of a part of the
                           <q>spine</q> in an early post-processing stage, featuring a passage in which Victor Frankenstein
                           beholds his completed Creature in each of the five editions. Notice that the <code class="code">&lt;rdg&gt;</code> elements contain mixed content: each is holding a passage from the source edition
                           that is crucially <span class="ital">not normalized</span>. This is our project’s particular approach to the <span class="ital">parallel segmentation</span> method of preparing a critical apparatus in TEI P5. Eventually the <q>spine</q> of the collation needs to contain data particular to each version in order to reconstruct
                           that version so that it stores information about its variations from the other versions.
                           In the not-quite-TEI stage of our post-collation process represented here, we can
                           see that a use of the Trojan milestone markers to indicate the starting points of
                           paragraphs, and their values indicate their location in the original source edition's
                           XML hierarchy.<sup class="fn-label"><a href="#d2e337" class="footnoteref" id="d2e337-ref">[7]</a></sup>
                           
                           
                           <pre class="programlisting" id="d2e346"><!--language: xml-->   
    &lt;cx:apparatus xmlns:cx="http://interedition.eu/collatex/ns/1.0"&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['chapter']"&gt;
			      &lt;rdg wit="f1818"&gt;&lt;milestone n="4" type="start" unit="chapter"/&gt;
			         &lt;head sID="novel1_letter4_chapter4_div4_div4_head1"/&gt;CHAPTER &lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;&lt;milestone n="4" type="start" unit="chapter"/&gt;
			         &lt;head sID="novel1_letter4_chapter4_div4_div4_head1"/&gt;CHAPTER &lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;&lt;milestone n="4" type="start" unit="chapter"/&gt;
			         &lt;head sID="novel1_letter4_chapter4_div4_div4_head1"/&gt;CHAPTER &lt;/rdg&gt;
			      &lt;rdg wit="f1831"&gt;&lt;milestone n="5" type="start" unit="chapter"/&gt;
			         &lt;head sID="novel1_letter4_chapter5_div4_div5_head1"/&gt;CHAPTER &lt;/rdg&gt;
			      &lt;rdg wit="fMS"&gt;&lt;lb n="c56-0045__main__1"/&gt;
			         &lt;milestone spanTo="#c56-0045.04" unit="tei:head"/&gt;Chapter &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['7&lt;shi rend="sup"&gt;&lt;/shi&gt;&lt;p/&gt;']"&gt;
			      &lt;rdg wit="fMS"&gt;7&lt;shi rend="sup"&gt;&lt;/shi&gt;
			         &lt;milestone unit="tei:p"/&gt;&lt;lb n="c56-0045__main__2"/&gt; &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
		    &lt;rdgGrp n="['iv.&lt;p/&gt;']"&gt;
			      &lt;rdg wit="f1818"&gt;IV.&lt;head eID="novel1_letter4_chapter4_div4_div4_head1"/&gt;
			         &lt;p sID="novel1_letter4_chapter4_div4_div4_p1"/&gt;&lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;IV.&lt;head eID="novel1_letter4_chapter4_div4_div4_head1"/&gt;
			         &lt;p sID="novel1_letter4_chapter4_div4_div4_p1"/&gt;&lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;IV.&lt;head eID="novel1_letter4_chapter4_div4_div4_head1"/&gt;
			         &lt;p sID="novel1_letter4_chapter4_div4_div4_p1"/&gt;&lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
		    &lt;rdgGrp n="['v.&lt;p/&gt;']"&gt;
			      &lt;rdg wit="f1831"&gt;V.&lt;head eID="novel1_letter4_chapter5_div4_div5_head1"/&gt;
			         &lt;p sID="novel1_letter4_chapter5_div4_div5_p1"/&gt; &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['it']"&gt;
			      &lt;rdg wit="f1818"&gt;I&lt;hi sID="novel1_letter4_chapter4_div4_div4_p1_hi1"/&gt;T
			         &lt;hi eID="novel1_letter4_chapter4_div4_div4_p1_hi1"/&gt; &lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;I&lt;hi sID="novel1_letter4_chapter4_div4_div4_p1_hi1"/&gt;T
			         &lt;hi eID="novel1_letter4_chapter4_div4_div4_p1_hi1"/&gt; &lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;I&lt;hi sID="novel1_letter4_chapter4_div4_div4_p1_hi1"/&gt;T
			         &lt;hi eID="novel1_letter4_chapter4_div4_div4_p1_hi1"/&gt; &lt;/rdg&gt;
			      &lt;rdg wit="f1831"&gt;I&lt;hi sID="novel1_letter4_chapter5_div4_div5_p1_hi1"/&gt;T
			         &lt;hi eID="novel1_letter4_chapter5_div4_div5_p1_hi1"/&gt; &lt;/rdg&gt;
			      &lt;rdg wit="fMS"&gt;It &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['was', 'on', 'a', 'dreary', 'night', 'of']"&gt;
			      &lt;rdg wit="f1818"&gt;was on a dreary night of &lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;was on a dreary night of &lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;was on a dreary night of &lt;/rdg&gt;
			      &lt;rdg wit="f1831"&gt;was on a dreary night of &lt;/rdg&gt;
			      &lt;rdg wit="fMS"&gt;was on a dreary night of &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['november', '']"&gt;
			      &lt;rdg wit="fMS"&gt;November &lt;lb n="c56-0045__main__3"/&gt; &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
		    &lt;rdgGrp n="['november,']"&gt;
			      &lt;rdg wit="f1818"&gt;November, &lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;November, &lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;November, &lt;/rdg&gt;
			      &lt;rdg wit="f1831"&gt;November, &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['that', 'i', 'beheld']"&gt;
			      &lt;rdg wit="f1818"&gt;that I beheld &lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;that I beheld &lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;that I beheld &lt;/rdg&gt;
			      &lt;rdg wit="f1831"&gt;that I beheld &lt;/rdg&gt;
			      &lt;rdg wit="fMS"&gt;that I beheld &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
	  &lt;app&gt;
		    &lt;rdgGrp n="['&lt;del&gt;the', 'frame', 'on', 'whic&lt;del&gt;',
		    'my', 'man', 'completeed,.', '&lt;del&gt;and&lt;del&gt;']"&gt;
			      &lt;rdg wit="fMS"&gt;&lt;del rend="strikethrough" sID="c56-0045__main__d2e9718"/&gt;
			         the frame on whic&lt;del eID="c56-0045__main__d2e9718"/&gt; my man 
			         comple&lt;mdel&gt;at&lt;/mdel&gt;teed,. 
			         &lt;del rend="strikethrough" sID="c56-0045__main__d2e9739"/&gt;
			         And&lt;del eID="c56-0045__main__d2e9739"/&gt;&lt;lb n="c56-0045__main__4"/&gt; &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
		    &lt;rdgGrp n="['the', 'accomplishment', 'of']"&gt;
			      &lt;rdg wit="f1818"&gt;the accomplishment of my toils. &lt;/rdg&gt;
			      &lt;rdg wit="f1823"&gt;the accomplishment of my toils. &lt;/rdg&gt;
			      &lt;rdg wit="fThomas"&gt;the accomplishment of my toils. &lt;/rdg&gt;
			      &lt;rdg wit="f1831"&gt;the accomplishment of my toils. &lt;/rdg&gt;
		    &lt;/rdgGrp&gt;
	  &lt;/app&gt;
                    </pre>
                           </p>
                     </li>
                     <li id="d2e349">
                        <p id="d2e350">Apply this collation data in post-processing to prepare the edition files:
                           
                           <div class="itemizedlist" id="d2e352">
                              <ul>
                                 <li id="d2e353">
                                    <p id="d2e354"><q>Raise</q> the flattened editions in their original form (convert Trojan milestones into the
                                       original tree hierarchy).</p>
                                 </li>
                                 <li id="d2e357">
                                    <p id="d2e358">With XSLT, pull information from the <q>spine</q> file, and insert  <code class="code">&lt;seg&gt;</code> elements in each of the distinct edition files to mark every point of variance captured
                                       in the collation. Each <code class="code">&lt;seg&gt;</code> element is mapped to a specific location in the collation <q>spine</q> file. This mapping is done by recording in the value of the <code class="code">seg @xml:id</code> a modified XPath expression for the particular <code class="code">&lt;app&gt;</code> element in the collation spine. (The modification to the XPath is necessary so that
                                       the result is valid as an xml:id, which may not contain a solidus.) Here is an example
                                       of <code class="code">&lt;seg&gt;</code> elements applied in the 1818 edition file. Here the <code class="code">@xml:id</code> attributes are keyed to the specific <code class="code">&lt;app&gt;</code> elements in the portion of the spine representing collation unit 10. Where the collation
                                       <q>spine</q> marks passages around chapter headings and paragraph boundaries, we apply a <code class="code">@part</code> attribute to designate an initial and final portion within the structure of the edition
                                       XML file:
                                       
                                       <pre class="programlisting" id="d2e384"><!--language: xml-->
         &lt;div type="collation"&gt;
            &lt;milestone n="4" type="start" unit="chapter"/&gt;
            &lt;head xml:id="novel1_letter4_chapter4_div4_div4_head1"&gt;
                CHAPTER &lt;seg part="I" xml:id="C10_app2-f1818__I"&gt;IV.&lt;/seg&gt;
            &lt;/head&gt;
            &lt;p xml:id="novel1_letter4_chapter4_div4_div4_p1"&gt;
               &lt;seg part="F" xml:id="C10_app2-f1818__F"&gt;
               &lt;/seg&gt;I&lt;hi xml:id="novel1_letter4_chapter4_div4_div4_p1_hi1"&gt;T&lt;/hi&gt;
                  was on a dreary night of &lt;seg xml:id="C10_app5-f1818"&gt;November, &lt;/seg&gt;
                  that I beheld &lt;seg xml:id="C10_app7-f1818"&gt;the accomplishment of my toils. &lt;/seg&gt;
                  With an anxiety that almost amounted to &lt;seg xml:id="C10_app9-f1818"&gt;agony, &lt;/seg&gt;
                  I collected &lt;seg xml:id="C10_app11-f1818"&gt;the &lt;/seg&gt;instruments of life around 
                  &lt;seg xml:id="C10_app14-f1818"&gt;me, that &lt;/seg&gt;I 
                  &lt;seg xml:id="C10_app16-f1818"&gt;might infuse &lt;/seg&gt;a spark of being 
                  into the lifeless thing that lay at my feet. . . .
            &lt;/p&gt; 
             . . . 
         &lt;/div&gt;
                            </pre>
                                       </p>
                                 </li>
                                 <li id="d2e387">
                                    <p id="d2e388">Convert the text contents of the <q>spine</q> critical apparatus into stand-off pointers to the <code class="code">&lt;seg&gt;</code> elements in the new edition files. Here is a passage from the converted <q>standoff spine</q> featuring only the 
                                       normalized tokens for each variant passage, and containing pointers to the locations
                                       of <code class="code">&lt;seg&gt;</code>
                                       elements in each edition XML file corresponding with a reading witness:
                                       
                                       <pre class="programlisting" id="d2e398"><!--language: xml-->
            &lt;app xml:id="C10_app7" n="48"&gt;
               &lt;rdgGrp xml:id="C10_app7_rg1"
                       n="['&lt;del&gt;the', 'frame', 'on', 'whic&lt;del&gt;', 'my', 'man', 'completeed,.', '&lt;del&gt;and&lt;del&gt;']"&gt;
                  &lt;rdg wit="#fMS"&gt;
                     &lt;ptr target="https://raw.githubusercontent.com/PghFrankenstein/fv-data/master/variorum-chunks/fMS_C10.xml#string-range(//tei:surface[@xml:id='ox-ms_abinger_c56-0045']/tei:zone[@type='main']//tei:line[3],15,59)"/&gt;
                  &lt;/rdg&gt;
               &lt;/rdgGrp&gt;
               &lt;rdgGrp xml:id="C10_app7_rg2" n="['the', 'accomplishment', 'of']"&gt;
                  &lt;rdg wit="#f1818"&gt;
                     &lt;ptr target="https://raw.githubusercontent.com/PghFrankenstein/fv-data/master/variorum-chunks/f1818_C10.xml#C10_app7-f1818"/&gt;
                  &lt;/rdg&gt;
                  &lt;rdg wit="#f1823"&gt;
                     &lt;ptr target="https://raw.githubusercontent.com/PghFrankenstein/fv-data/master/variorum-chunks/f1823_C10.xml#C10_app7-f1823"/&gt;
                  &lt;/rdg&gt;
                  &lt;rdg wit="#fThomas"&gt;
                     &lt;ptr target="https://raw.githubusercontent.com/PghFrankenstein/fv-data/master/variorum-chunks/fThomas_C10.xml#C10_app7-fThomas"/&gt;
                  &lt;/rdg&gt;
                  &lt;rdg wit="#f1831"&gt;
                     &lt;ptr target="https://raw.githubusercontent.com/PghFrankenstein/fv-data/master/variorum-chunks/f1831_C10.xml#C10_app7-f1831"/&gt;
                  &lt;/rdg&gt;
               &lt;/rdgGrp&gt;
            &lt;/app&gt;
                            
                            </pre>
                                       </p>
                                 </li>
                                 <li id="d2e401">
                                    <p id="d2e402">Calculate special links and pointers to the original XML files of the Shelley-Godwin
                                       Archive's manuscript edition of Frankenstein, adjusting for the resequencing prior
                                       to collation.</p>
                                 </li>
                              </ul>
                           </div>
                           </p>
                     </li>
                     <li id="d2e404">
                        <p id="d2e405">Publish the XML via a web framework (optimally CETEIcean, but <a href="https://frankensteinvariorum.github.io/viewer/" class="link">for now using Node.js and React</a>). Share the edition text files and XML data from our <a href="https://github.com/FrankensteinVariorum/fv-data" class="link">GitHub repo</a>.
                           
                           <figure id="fvwebview">
                              <p class="title">FV-webView: Web view of a variant passage in the <span class="ital">Frankenstein Variorum</span> reading interface</p>
                              <div class="figure-contents">
                                 <div class="mediaobject" id="d2e419"><img alt="" src="Bal2022Besh-FVarWebView.png" style="width: 80%"></div>
                                 <div class="caption">
                                    <p id="d2e423">A snapshot of the <span class="ital">Frankenstein Variorum</span> web interface showing the 1816 manuscript notebook edition at the moment when Victor
                                       Frankenstein looks at the form of his complete Creature, highlighting the variant
                                       passage featured in the preceding code blocks. The segmented passages highlight the
                                       variants with deepening shades based on Levenshtein distance measurements, divided
                                       into 3 ranges (very light for Levenshtein distances of a few characters, mid-range,
                                       and more intense for distances above 20).</p>
                                 </div>
                              </div>
                           </figure>
                           <sup class="fn-label"><a href="#d2e429" class="footnoteref" id="d2e429-ref">[8]</a></sup>
                           </p>
                     </li>
                  </ol>
               </div>
               </p>
         </div>
         <div class="section" id="d2e440">
            <h2 class="title" style="clear: both">Snags in the Weaving</h2>
            <div class="section" id="d2e442">
               <h3 class="title" style="clear: both">A Schematron flashlight</h3>
               <p id="d2e444">For an unhappy time two years ago, pressed by a deadline to launch a proof-of-concept
                  partial version of the Variorum interface, I hand-corrected the outputs of collation
                  assisted by a Schematron file that would help me to identify common problems. The
                  Schematron was like a flashlight guiding me through a forest of code in efforts to
                  revise the contents of <code class="code">&lt;rdg&gt;</code> elements and refine the organization of <code class="code">&lt;app&gt;</code> elements and their constituent <code class="code">&lt;rdgGrp&gt;</code> elements. The sheer volume of corrections was exhausting, and a short novel like
                  <span class="ital">Frankenstein</span> could seem infinitely long and tortuously slow to read while attempting to make <q>spine adjustments</q>. I knew this method was not sufficient for our needs. This Schematron file was merely
                  my first attempt to try to address the problem systematically:
                  
                  <pre class="programlisting" id="d2e458"><!--language: xml-->
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;sch:schema xmlns:sch="http://purl.oclc.org/dsdl/schematron" queryBinding="xslt2"&gt;
    &lt;sch:pattern&gt;
        &lt;sch:rule context="app"&gt;
            &lt;sch:report test="not(rdgGrp)" role="error"&gt;Empty app element--missing rdgGrps! That's an error introduced from editing the collation.&lt;/sch:report&gt;
     &lt;sch:report test="contains(descendant::rdg[@wit='fThomas'], '&lt;del')" role="info"&gt;
         Here is a place where the Thomas text contains a deleted passage. Is it completely encompassed in the app?&lt;/sch:report&gt;
            &lt;sch:assert test="count(descendant::rdg/@wit) = count(distinct-values(descendant::rdg/@wit))" role="error"&gt;
                A repeated rdg witness is present! There's an error here introduced by editing the collation.
            &lt;/sch:assert&gt;
        &lt;/sch:rule&gt;
    &lt;/sch:pattern&gt;
    &lt;sch:pattern&gt;
        &lt;sch:rule context="app[count(rdgGrp) eq 1][count(descendant::rdg) eq 1]"&gt;
            &lt;sch:report test="count(preceding-sibling::app[1]/rdgGrp) eq 1 or count(following-sibling::app[1]/rdgGrp) eq 1 or last()" role="warning"&gt;
                Here is a "singleton" app that may be best merged in with the preceding or following "unison" app as part of a new rdgGrp. 
            &lt;/sch:report&gt;
        &lt;/sch:rule&gt;
    &lt;/sch:pattern&gt;
    &lt;sch:pattern&gt;
        &lt;sch:let name="delString" value="'&lt;del'"/&gt;
    &lt;sch:rule context="rdg[@wit='fThomas']" role="error"&gt;
        &lt;sch:let name="textTokens" value="tokenize(text(), ' ')"/&gt;
        &lt;sch:let name="delMatch" value="for $t in $textTokens return $t[contains(., $delString)]"/&gt;
        &lt;sch:assert test="count($delMatch) mod 2 eq 0"&gt;
            Unfinished deletion in the Thomas witness. We count &lt;sch:value-of select="count($delMatch)"/&gt; deletion matches. 
            Make sure the Thomas witness deletion is completely encompassed in the app.&lt;/sch:assert&gt;
    &lt;/sch:rule&gt;
    &lt;/sch:pattern&gt;
    &lt;sch:pattern&gt;
        &lt;sch:rule context="rdgGrp[ancestor::rdgGrp]"&gt;
            &lt;sch:report test="."&gt;A reading group must NOT be nested inside another reading group!&lt;/sch:report&gt;
        &lt;/sch:rule&gt;
    &lt;/sch:pattern&gt;
&lt;/sch:schema&gt;</pre>   
                  When reviewing and editing collation <q>spine</q> files, I would apply this schema to guide my work and stop me from making terrible
                  mistakes in hand-correcting common collation output problems, such as, for example,
                  pasting a <code class="code">&lt;rdgGrp&gt;</code> element inside another <code class="code">&lt;rdgGrp&gt;</code> (permissible and useful in some TEI projects, but an outright mistake in our very
                  simple spine). Eye fatigue is a serious problem in just about every stage of the Gothenburg
                  model, and this Schematron did intervene to prevent clumsy mistakes. More importantly
                  for the long-range work of the project, this simple Schematron file helped me to begin
                  documenting and describing repeating patterns of error in the collation, most significantly
                  here the <q>singleton</q> <code class="code">&lt;app&gt;</code>, containing only one witness inside, and the incommplete witness that I wanted to
                  include a complete act of deletion.
                  </p>
            </div>
            <div class="section" id="d2e471">
               <h3 class="title" style="clear: both">XSLT for alignment correction</h3>
               <p id="d2e473">Revisiting the collation output in fall 2021 with two student research assistants,
                  we began identifying patterns that XSLT could correct. The output from collateX output
                  would frequently create a problem we called <q>loner dels</q>: that is, failing to align text with flattened <code class="code">&lt;del&gt;</code> markup wtih corresponding witnesses, despite the presence of related content with
                  other witnesses. collateX would sometimes place these in an <code class="code">&lt;app&gt;</code> with a single <code class="code">&lt;rdgGrp&gt;</code> by themselves. We succeeded over a few Friday afternoon research team meetings in
                  November to correct this while my students learned about tunneling parameters in XSLT.
                  Our Stylesheet is short but represents a step forward in documentation as well as
                  outright repair of collation alignment:</p>
               <pre class="programlisting" id="d2e483"><!--language: xml-->
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;xsl:stylesheet xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
    xmlns:xs="http://www.w3.org/2001/XMLSchema"
    xmlns:math="http://www.w3.org/2005/xpath-functions/math"
    xmlns:cx="http://interedition.eu/collatex/ns/1.0"
    
    exclude-result-prefixes="xs math"
    version="3.0"&gt;
    &lt;!--2021-09-24 ebb with wdjacca and amoebabyte: We are writing XSLT to try to move
    solitary apps reliably into their neighboring app elements representing all witnesses. 
    
    --&gt;
  &lt;xsl:mode on-no-match="shallow-copy"/&gt;
  
&lt;!-- ********************************************************************************************
        LONER DELS: These templates deal with collateX output of app elements 
        containing a solitary MS witness containing a deletion, which we interpret as usually a false start, 
        before a passage.
     *********************************************************************************************
    --&gt;  
    &lt;xsl:template match="app[count(descendant::rdg) = 1][contains(descendant::rdg, '&lt;del')]"&gt;
  
        &lt;xsl:if test="following-sibling::app[1][count(descendant::rdgGrp) = 1 and count(descendant::rdg) gt 1]"&gt;
               &lt;xsl:apply-templates select="following-sibling::app[1]" mode="restructure"&gt;
                  &lt;xsl:with-param as="node()" name="loner" select="descendant::rdg" tunnel="yes" /&gt;
                   &lt;xsl:with-param as="attribute()" name="norm" select="rdgGrp/@n" tunnel="yes"/&gt;
               &lt;/xsl:apply-templates&gt;
               
           &lt;/xsl:if&gt;
    &lt;/xsl:template&gt;
    
    
    &lt;xsl:template match="app[preceding-sibling::app[1][count(descendant::rdg) = 1][contains(descendant::rdg, '&lt;del')]]"/&gt;


    &lt;xsl:template match="app" mode="restructure"&gt;
        &lt;xsl:param name="loner" tunnel="yes"/&gt;
        &lt;xsl:param name="norm" tunnel="yes"/&gt;
        &lt;app&gt;
        &lt;xsl:apply-templates select="rdgGrp" mode="restructure"&gt;
                &lt;xsl:with-param  as="node()" name="loner" tunnel="yes" select="$loner"/&gt;
            &lt;/xsl:apply-templates&gt;
            &lt;xsl:variable name="TokenSquished"&gt;
                &lt;xsl:value-of select="$norm ! string()||descendant::rdgGrp[descendant::rdg[@wit=$loner/@wit]]/@n"/&gt;
            &lt;/xsl:variable&gt;
            &lt;xsl:variable name="newToken"&gt;
                &lt;xsl:value-of select="replace($TokenSquished, '\]\[', ', ')"/&gt;
            &lt;/xsl:variable&gt;
           &lt;rdgGrp n="{$newToken}"&gt;
              &lt;rdg wit="{$loner/@wit}"&gt;&lt;xsl:value-of select="$loner/text()"/&gt;
              &lt;xsl:value-of select="descendant::rdg[@wit = $loner/@wit]"/&gt;
              &lt;/rdg&gt;
               
           &lt;/rdgGrp&gt; 
        &lt;/app&gt; 
    &lt;/xsl:template&gt;
    
    &lt;xsl:template match="rdgGrp" mode="restructure"&gt;
        &lt;xsl:param name="loner" tunnel="yes"/&gt;

           &lt;xsl:if test="rdg[@wit ne $loner/@wit]"&gt;
            &lt;xsl:copy-of select="current()" /&gt;
        &lt;/xsl:if&gt;
    &lt;/xsl:template&gt;
&lt;/xsl:stylesheet&gt;
</pre>
               <p id="d2e485">Next my little team and I began to tackle the noxious problem of <q>empty tokens</q>, that is, spurious <code class="code">&lt;app&gt;</code> elements that would form around a single witness, the Shelley-Godwin Archive's manuscript
                  witness. Most frequently, these isolated <code class="code">&lt;app&gt;</code> elements would with an empty token, <code class="code">""</code>, formed to represent the normalized form  a <code class="code">&lt;lb/&gt;</code> element. The empty token itself is quite correct for our collation normalization
                  process: The Shelley-Godwin Archive editors applied line-by-line encoding of the manuscript
                  notebook, and we need to preserve the information about each line in order to help
                  ensure that each passage is connected back to its source edition file. So in our project,
                  we must not ignore this markup, but we do normalize it for the purposes of the collation
                  since, in our document data model, it is not meaningful information about how the
                  manuscript notebooks compare with the printed editions. In pre-processing the source
                  files for collation, we converted the Shelley-Godwin Archive’s <code class="code">&lt;line&gt;</code> elements into empty milestone markers as <code class="code">&lt;lb/&gt;</code> marking the beginnings of each line, and these <code class="code">&lt;lb&gt;</code> elements with their informative attributes are output as contents of <code class="code">&lt;rdg wit="fMS"&gt;</code> in the <q>spine</q> file. Our normalizing algorithm converts them into empty tokens to indicate that
                  they are meaningless for the purposes of collation, but they are nevertheless meaningful
                  to the reconstruction of the 1816 manuscript edition information following collation.</p>
               <p id="d2e508">In their most benign and frequently occurring form, the empty-token problem would
                  appear in separate isolated <code class="code">&lt;app&gt;</code> elements in between two <q>unison apps</q> (that is, interrupting a passage in which all witnesses should align in unison. Here
                  is an example of the pattern. Those reading vigilantly will note that the expected
                  octothorpe or sharp <code class="code">#</code> characters indicating that this is a pointer to a defined <code class="code">@xml:id</code> do not appear in the <code class="code">@wit</code> attribute values in this example. These are simply lacking at their point of output
                  from collateX, before it is reprocessed in the post-production pipeline as a TEI document.
                  
                  
                  <pre class="programlisting" id="d2e521"><!--language: xml-->
     &lt;app&gt;
		&lt;rdgGrp n="['as', 'i', 'did', 'not', 'appear', 'to', 'know']"&gt;
			&lt;rdg wit="f1818"&gt;as I did not appear to know &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;as I did not appear to know &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;as I did not appear to know &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;as I did not appear to know &lt;/rdg&gt;
			&lt;rdg wit="fMS"&gt;as I did not appear to know &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
	&lt;app&gt;
		&lt;rdgGrp n="['']"&gt;
			&lt;rdg wit="fMS"&gt;&lt;lb n="c57-0119__main__14"/&gt; &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
	&lt;app&gt;
		&lt;rdgGrp n="['the']"&gt;
			&lt;rdg wit="f1818"&gt;the &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;the &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;the &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;the &lt;/rdg&gt;
			&lt;rdg wit="fMS"&gt;the &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;</pre>
                  The <q>empty token</q> problem is evident here in the normalized form of an <code class="code">&lt;lb/&gt;</code> element with all its attributes, being read properly as an array of one item, a null
                  string, but somehow despite being nullified, asserting its presence to interrupt a
                  unison passage. At this point, ideally there will be only one <code class="code">&lt;app&gt;</code> element that should appear like this:
                  
                  <pre class="programlisting" id="d2e530"><!--language: xml-->
            &lt;app&gt;
		&lt;rdgGrp n="['as', 'i', 'did', 'not', 'appear', 'to', 'know', 'the']"&gt;
			&lt;rdg wit="f1818"&gt;as I did not appear to know the &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;as I did not appear to know the &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;as I did not appear to know the &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;as I did not appear to know the &lt;/rdg&gt;
			&lt;rdg wit="fMS"&gt;as I did not appear to know 
			 &lt;rdg wit="fMS"&gt;&lt;lb n="c57-0119__main__14"/&gt; the &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
        </pre>
                  There is a more complex form of this pernicious problem with collating around normalized
                  markup, and that is when the token asserts itself to generate a delta, a separate
                  <code class="code">&lt;rdgGrp&gt;</code> within an app, as happens here:
                  
                  <pre class="programlisting" id="d2e536"><!--language: xml-->
    &lt;app&gt;
		&lt;rdgGrp n="['departed.', 'besides,', 'they']"&gt;
			&lt;rdg wit="f1818"&gt;departed. Besides, they &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;departed. Besides, they &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;departed. Besides, they &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;departed. Besides, they &lt;/rdg&gt;
			&lt;rdg wit="fMS"&gt;departed. Besides, they &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
	&lt;app&gt;
		&lt;rdgGrp n="['observed']"&gt;
			&lt;rdg wit="f1818"&gt;observed &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;observed &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;observed &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;observed &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
		&lt;rdgGrp n="['observed,', '']"&gt;
			&lt;rdg wit="fMS"&gt;observed, &lt;lb n="c57-0119__main__11"/&gt; &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
	&lt;app&gt;
		&lt;rdgGrp n="['that', 'it', 'appeared']"&gt;
			&lt;rdg wit="f1818"&gt;that it appeared &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;that it appeared &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;that it appeared &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;that it appeared &lt;/rdg&gt;
			&lt;rdg wit="fMS"&gt;that it appeared &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
        </pre>
                  The capacity of the empty token to create a <q>snag</q> in the collation weave is evident here, generating a false divergence. These <code class="code">&lt;app&gt;</code>s should all be consolidated into one element wth just one unison <code class="code">&lt;rdgGrp&gt;</code>. My research team and I took notes on the various forms this empty token phenomenon
                  could take and drafted notes, still on my office whiteboard, for how we could handle
                  them with XSLT.
                  </p>
               <figure id="fvwhiteboard">
                  <p class="title">FV-whiteboard: Frankenstein collation notes on my office whiteboard</p>
                  <div class="figure-contents">
                     <div class="mediaobject" id="d2e547"><img alt="" src="Bal2022Besh-FVWhiteBoard.jpg" style="width: 80%"></div>
                     <div class="caption">
                        <p id="d2e551">My student Jacqueline Chan, Penn State Behrend May 2022 Digital Media, Arts, and Technology
                           graduate, carefully drafted these notes on the <q>empty token</q> collation problem and our efforts to solve it on my office whiteboard during a meeting
                           with my student research assistants as we inspected collation output code together.</p>
                     </div>
                  </div>
               </figure>
            </div>
            <div class="section" id="d2e555">
               <h3 class="title" style="clear: both">The <q>smashed-tokens</q> problem</h3>
               <p id="d2e560">We discovered another particularly pernicious problem with tokenizing and normalizing
                  markup via our Python process to collateX. This problem is one of occasional <q>smashed-tokens</q>, that is two tokens losing their space separator and becoming one token. Here is
                  a very short passage of XML from the Shelley-Godwin Archive's encoding:
                  
                  <pre class="programlisting" id="d2e564"><!--language: xml-->
                 . . . the spot&lt;add eID="c57-0117__main__d3e21951"/&gt; &amp; endeavoured . . .   
               </pre>
                  In this passage, we will be normalizing to ignore the <code class="code">&lt;add&gt;</code> element. Notice the spaces separating the element tag from the ampersand. Now, here
                  is how collateX interprets the passage in its collation alignment:
                  
                  <pre class="programlisting" id="d2e569"><!--language: xml-->
    &lt;app&gt;
		&lt;rdgGrp n="['spot,', 'and', 'endeavoured,']"&gt;
			&lt;rdg wit="f1818"&gt;spot, and endeavoured, &lt;/rdg&gt;
			&lt;rdg wit="f1823"&gt;spot, and endeavoured, &lt;/rdg&gt;
			&lt;rdg wit="fThomas"&gt;spot, and endeavoured, &lt;/rdg&gt;
			&lt;rdg wit="f1831"&gt;spot, and endeavoured, &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
		&lt;rdgGrp n="['spotand', 'endeavoured']"&gt;
			&lt;rdg wit="fMS"&gt;spot&amp; endeavoured &lt;/rdg&gt;
		&lt;/rdgGrp&gt;
	&lt;/app&gt;
               </pre>
                  
                  Somewhere in the normalizing process, the two completely separate tokens <q>spot</q> and <q>and</q> were spliced together into one token. We are not certain whether this problem is
                  more with Python's tokenization or with collateX's alignment of normalized empty strings
                  representing markup irrelevant to the collation. It's as if the normalized markup
                  refuses quite to disappear, haunting the collation machinery with a ghostly persistent
                  trace. We are also not certain at the date of this writing whether choosing a different
                  alignment algorithm in collateX might resolve the problem, but I think not, since
                  error seems to be in interpreting spaces following normalization. That is, I believe
                  the problems could be in the Python script that handles the normalization.</p>
               <p id="d2e577">At the date of this writing, we have not completed these XSLT post-processing tasks,
                  in part because we all became busy wih senior projects (for them, completing their
                  projects and for me, assisting with them). I have also paused a moment before continuing
                  down this path because the more patterns we discovered to correct, the more I wanted
                  to try another XSLT approach. If our normalization and alignment were so disjointed
                  to require so much XSLT post-processing, what if XSLT might simply be better at the
                  task of normalizing markup and tokenizing and aligning around it? That is, would it
                  be less brittle to use XSLT for all of our processing, including the collation itself?
                  </p>
            </div>
         </div>
         <div class="section" id="d2e578">
            <h2 class="title" style="clear: both">XSLT for the win?</h2>
            <p id="d2e580">At the August 2021 Balisage conference, Joel Kalvesmaki introduced <code class="code">tan:diff</code> and <code class="code">tan:collate</code>. As I listened to the paper, I was struck by how clear, simple, and precise Kalvesmaki
               made the process of string comparison seem, using XPath and XSLT:  
               <div class="blockquote">
                  <blockquote class="blockquote">
                     <p id="d2e587"><code class="code">tan:diff()</code> . . . extracts a series of progressively smaller samples from the shorter of the
                        two texts and looks for a match in the longer one on the basis of <code class="code">fn:contains()</code>. When a match is found, the results are separated by <code class="code">fn:substring-before()</code> and <code class="code">fn:substring-after()</code><sup class="fn-label"><a href="#d2e595" class="footnoteref" id="d2e595-ref">[9]</a></sup></p>
                  </blockquote>
               </div>
               The application of XPath functions described here made the alignment process seem
               more familiar and accessible, amenable to the kinds of modifications I regularly make
               with my own XSLT. I wondered whether starting with attempts to match longer samples
               and moving to match shorter ones might generate fewer aligned clusters and reduce
               the spurious particulation I had been seeing in my Python-assisted process with collateX.
               So I have lately begun experimenting with <code class="code">tan:diff()</code> and <code class="code">tan:collate()</code>.
               </p>
            <p id="d2e611">At the time of this writing, I have begun testing a collation unit from the <span class="ital">Frankenstein Variorum</span>, the same featured above that produced the <q>smashed tokens</q> problem featured in the previous section. Currently I am applying the software following
               Kalvesmaki’s detailed comments within his sample Diff XSLT files guiding the user
               to generate TAN Diff's native XML and HTML output. The XML output is like, but yet
               unlike the TEI critical apparatus structure we have been using in the <span class="ital">Frankenstein Variorum</span> project. The markup is simply organized in <code class="code">&lt;c&gt;</code> when all witnesses share a normalized string, and <code class="code">&lt;u&gt;</code> when they are variant from each other. There is nothing like the <code class="code">&lt;app&gt;</code> element to bundle together related groups. But this is the realm of XSLT and the
               developers encourage their users to adapt the code to their needs. Alas, however,
               at the time of this writing, it appears there may be no straightforward way for me,
               without assistance from the developer, to output the original text in the format we
               need for the <q>spine</q> of the <span class="ital">Frankenstein Variorum</span> project. This remains an area for future development.</p>
            <p id="d2e633">Nevertheless, I have been applying and updating my 25 normalizing replacement patterns
               via XSLT and find this much more legible than my handling of the same in my Python
               script (which sometimes contained duplicate passages). XSLT, by nature of being written
               in XML is XPathable, and with a large quantity of replacement sequences, I am able
               to check and sequence the process carefully in a way that is easier for me to document
               legibly.
               
               <pre class="programlisting" id="d2e635"><!--language: xml-->
                &lt;!-- Should punctuation be ignored? --&gt;
    &lt;xsl:param name="tan:ignore-punctuation-differences" as="xs:boolean" select="false()"/&gt;
    
    &lt;xsl:param name="additional-batch-replacements" as="element()*"&gt;
        &lt;!--ebb: normalizations to batch process for collation. NOTE: We want to do these to preserve some markup \\
            in the output for post-processing to reconstruct the edition files. 
            Remember, these will be processed in order, so watch out for conflicts. --&gt;
        &lt;replace pattern="(&lt;.+?&gt;\s*)&amp;gt;" replacement="$1" message="normalizing away extra right angle brackets"/&gt;
         &lt;replace pattern="&amp;amp;" replacement="and" message="ampersand batch replacement"/&gt;
        &lt;replace pattern="&lt;/?xml&gt;" replacement="" message="xml tag replacement"/&gt;
        &lt;replace pattern="(&lt;p)\s+.+?(/&gt;)" replacement="$1$2" message="p-tag batch replacement"/&gt;
        &lt;replace pattern="(&lt;)(metamark).*?(&gt;).+?\1/\2\3" replacement="" message="metamark batch replacement"/&gt;
        &lt;!--ebb: metamark contains a text node, and we don't want its contents processed in the collation, so this captures the entire element. --&gt;
        &lt;replace pattern="(&lt;/?)m(del).*?(&gt;)" replacement="$1$2$3" message="mdel-SGA batch replacement"/&gt;  
        &lt;!--ebb: mdel contains a text node, so this catches both start and end tag.
        We want mdel to be processed as &lt;del&gt;...&lt;/del&gt;--&gt;
        &lt;replace pattern="&lt;/?damage.*?&gt;" replacement="" message="damage-SGA batch replacement"/&gt; 
        &lt;!--ebb: damage contains a text node, so this catches both start and end tag. --&gt;
        &lt;replace pattern="&lt;/?unclear.*?&gt;" replacement="" message="unclear-SGA batch replacement"/&gt; 
        &lt;!--ebb: unclear contains a text node, so this catches both start and end tag. --&gt;
        &lt;replace pattern="&lt;/?retrace.*?&gt;" replacement="" message="retrace-SGA batch replacement"/&gt; 
            &lt;!--ebb: retrace contains a text node, so this catches both start and end tag. --&gt;
        &lt;replace pattern="&lt;/?shi.*?&gt;" replacement="" message="shi-SGA batch replacement"/&gt; 
        &lt;!--ebb: shi (superscript/subscript) contains a text node, so this catches both start and end tag. --&gt;
        &lt;replace pattern="(&lt;del)\s+.+?(/&gt;)" replacement="$1$2" message="del-tag batch replacement"/&gt;
        &lt;replace pattern="&lt;hi.+?/&gt;" replacement="" message="hi batch replacement"/&gt;
        &lt;replace pattern="&lt;pb.+?/&gt;" replacement="" message="pb batch replacement"/&gt;
        &lt;replace pattern="&lt;add.+?&gt;" replacement="" message="add batch replacement"/&gt;
        &lt;replace pattern="&lt;w.+?/&gt;" replacement="" message="w-SGA batch replacement"/&gt;
        &lt;replace pattern="(&lt;del)Span.+?spanTo="#(.+?)".*?(/&gt;)(.+?)&lt;anchor.+?xml:id="\2".*?&gt;" 
            replacement="$1$3$4$1$3" message="delSpan-to-anchor-SGA batch replacement"/
       &lt;replace pattern="&lt;anchor.+?/&gt;" replacement="" message="anchor-SGA batch replacement"/&gt;  
        &lt;replace pattern="&lt;milestone.+?unit="tei:p".+?/&gt;" replacement="&lt;p/&gt; &lt;p/&gt;" 
            message="milestone-paragraph-SGA batch replacement"/&gt;  
        &lt;replace pattern="&lt;milestone.+?/&gt;" replacement="" message="milestone non-p batch replacement"/&gt;  
        &lt;replace pattern="&lt;lb.+?/&gt;" replacement="" message="lb batch replacement"/&gt;  
        &lt;replace pattern="&lt;surface.+?/&gt;" replacement="" message="surface-SGA batch replacement"/&gt; 
        &lt;replace pattern="&lt;zone.+?/&gt;" replacement="" message="zone-SGA batch replacement"/&gt; 
        &lt;replace pattern="&lt;mod.+?/&gt;" replacement="" message="mod-SGA batch replacement"/&gt; 
        &lt;replace pattern="&lt;restore.+?/&gt;" replacement="" message="restore-SGA batch replacement"/&gt; 
        &lt;replace pattern="&lt;graphic.+?/&gt;" replacement="" message="graphic-SGA batch replacement"/&gt; 
        &lt;replace pattern="&lt;head.+?/&gt;" replacement="" message="head batch replacement"/&gt; 
 
            </pre>
               
               The documentation-centered nature of the TAN stylesheets makes the XSLT function to
               document decisions and makes it easy to amend the process and resequence it. Contrast
               this with the brittle complexity of my Python normalization function and its dependencies:
               
               
               <pre class="programlisting" id="d2e638"><!--language: python-->
regexWhitespace = re.compile(r'\s+')
regexNonWhitespace = re.compile(r'\S+')
regexEmptyTag = re.compile(r'/&gt;$')
regexBlankLine = re.compile(r'\n{2,}')
regexLeadingBlankLine = re.compile(r'^\n')
regexPageBreak = re.compile(r'&lt;pb.+?/&gt;')
RE_MARKUP = re.compile(r'&lt;.+?&gt;')
RE_PARA = re.compile(r'&lt;p\s[^&lt;]+?/&gt;')
RE_INCLUDE = re.compile(r'&lt;include[^&lt;]*/&gt;')
RE_MILESTONE = re.compile(r'&lt;milestone[^&lt;]*/&gt;')
RE_HEAD = re.compile(r'&lt;head[^&lt;]*/&gt;')
RE_AB = re.compile(r'&lt;ab[^&lt;]*/&gt;')
# 2018-10-1 ebb: ampersands are apparently not treated in python regex as entities any more than angle brackets.
# RE_AMP_NSB = re.compile(r'\S&amp;\s')
# RE_AMP_NSE = re.compile(r'\s&amp;\S')
# RE_AMP_SQUISH = re.compile(r'\S&amp;\S')
# RE_AMP = re.compile(r'\s&amp;\s')
RE_AMP = re.compile(r'&amp;')
# RE_MULTICAPS = re.compile(r'(?&lt;=\W|\s|\&gt;)[A-Z][A-Z]+[A-Z]*\s')
# RE_INNERCAPS = re.compile(r'(?&lt;=hi\d"/&gt;)[A-Z]+[A-Z]+[A-Z]+[A-Z]*')
# TITLE_MultiCaps = match(RE_MULTICAPS).lower()
RE_DELSTART = re.compile(r'&lt;del[^&lt;]*&gt;')
RE_ADDSTART = re.compile(r'&lt;add[^&lt;]*&gt;')
RE_MDEL = re.compile(r'&lt;mdel[^&lt;]*&gt;.+?&lt;/mdel&gt;')
RE_SHI = re.compile(r'&lt;shi[^&lt;]*&gt;.+?&lt;/shi&gt;')
RE_METAMARK = re.compile(r'&lt;metamark[^&lt;]*&gt;.+?&lt;/metamark&gt;')
RE_HI = re.compile(r'&lt;hi\s[^&lt;]*/&gt;')
RE_PB = re.compile(r'&lt;pb[^&lt;]*/&gt;')
RE_LB = re.compile(r'&lt;lb.*?/&gt;')
# 2021-09-06: ebb and djb: On &lt;lb&gt; collation troubles: LOOK FOR DOT MATCHES ALL FLAG
# b/c this is likely spanning multiple lines, and getting split by the tokenizing algorithm.
# 2021-09-10: ebb with mb and jc: trying .*? and DOTALL flag
RE_LG = re.compile(r'&lt;lg[^&lt;]*/&gt;')
RE_L = re.compile(r'&lt;l\s[^&lt;]*/&gt;')
RE_CIT = re.compile(r'&lt;cit\s[^&lt;]*/&gt;')
RE_QUOTE = re.compile(r'&lt;quote\s[^&lt;]*/&gt;')
RE_OPENQT = re.compile(r'“')
RE_CLOSEQT = re.compile(r'”')
RE_GAP = re.compile(r'&lt;gap\s[^&lt;]*/&gt;')
# &lt;milestone unit="tei:p"/&gt;
RE_sgaP = re.compile(r'&lt;milestone\sunit="tei:p"[^&lt;]*/&gt;')

. . .

def normalize(inputText):
# 2018-09-23 ebb THIS WORKS, SOMETIMES, BUT NOT EVERWHERE: RE_MULTICAPS.sub(format(re.findall(RE_MULTICAPS, inputText, flags=0)).title(), \
# RE_INNERCAPS.sub(format(re.findall(RE_INNERCAPS, inputText, flags=0)).lower(), \
    return RE_MILESTONE.sub('', \
        RE_INCLUDE.sub('', \
        RE_AB.sub('', \
        RE_HEAD.sub('', \
        RE_AMP.sub('and', \
        RE_MDEL.sub('', \
        RE_SHI.sub('', \
        RE_HI.sub('', \
        RE_LB.sub('', \
        RE_PB.sub('', \
        RE_PARA.sub('&lt;p/&gt;', \
        RE_sgaP.sub('&lt;p/&gt;', \
        RE_LG.sub('&lt;lg/&gt;', \
        RE_L.sub('&lt;l/&gt;', \
        RE_CIT.sub('', \
        RE_QUOTE.sub('', \
        RE_OPENQT.sub('"', \
        RE_CLOSEQT.sub('"', \
        RE_GAP.sub('', \
        RE_DELSTART.sub('&lt;del&gt;', \
        RE_ADDSTART.sub('&lt;add&gt;', \
        RE_METAMARK.sub('', inputText)))))))))))))))))))))).lower()  
                </pre>            
               I came to realize that in the more writerly environment of the XSLT stylesheets for
               TAN Diff, It is easier to review the code, which would be helpful to me setitng it
               down and returning to it after some months or years. 
               </p>
            <p id="d2e641">More to the point of this experiment with a new alignment method, does it succeed
               in reducing the tokenization and noramlization alignments discussed in the previous
               section? So far, yes, often with identical correct alignments and usually with longer
               matches. For example, on the passage that illustrated the <q>smashed tokens</q> problem, here is tan collate's output: 
               
               <pre class="programlisting" id="d2e645"><!--language: xml-->
                &lt;u&gt;
         &lt;txt&gt;spot,&lt;/txt&gt;
         &lt;wit ref="2-1818_fullFlat_C27" pos="1423"/&gt;
         &lt;wit ref="4-Thomas_fullFlat_C27" pos="1423"/&gt;
         &lt;wit ref="3-1823_fullFlat_C27" pos="1421"/&gt;
         &lt;wit ref="5-1831_fullFlat_C27" pos="1422"/&gt;
      &lt;/u&gt;
      &lt;u&gt;
         &lt;txt&gt;spot&lt;/txt&gt;
         &lt;wit ref="1-msColl_C27" pos="317"/&gt;
      &lt;/u&gt;
      &lt;c&gt;
         &lt;txt&gt; and &lt;/txt&gt;
         &lt;wit ref="2-1818_fullFlat_C27" pos="1428"/&gt;
         &lt;wit ref="4-Thomas_fullFlat_C27" pos="1428"/&gt;
         &lt;wit ref="3-1823_fullFlat_C27" pos="1426"/&gt;
         &lt;wit ref="5-1831_fullFlat_C27" pos="1427"/&gt;
         &lt;wit ref="1-msColl_C27" pos="321"/&gt;
      &lt;/c&gt;
      &lt;u&gt;
         &lt;txt&gt;endeavoured,&lt;/txt&gt;
         &lt;wit ref="2-1818_fullFlat_C27" pos="1433"/&gt;
         &lt;wit ref="4-Thomas_fullFlat_C27" pos="1433"/&gt;
         &lt;wit ref="3-1823_fullFlat_C27" pos="1431"/&gt;
         &lt;wit ref="5-1831_fullFlat_C27" pos="1432"/&gt;
      &lt;/u&gt;
      &lt;u&gt;
         &lt;txt&gt;endeavoured &lt;/txt&gt;
         &lt;wit ref="1-msColl_C27" pos="326"/&gt;
      &lt;/u&gt;     
            </pre>
               
               I have found no problems with empty tokens created from markup, and overall the number
               of aligned groups is much smaller. Here is a summary of total aligned reading groups
               and unison aligned reading groups for the same collation unit as processed by Python-mediated
               CollateX vs. Tan Diff XSLT:
               
               
               <div class="table-wrapper" id="mul-table1">
                  <p class="title">Table&nbsp;I</p>
                  <div class="caption">
                     <p id="d2e650">Alignments generated by CollateX vs. Tan Diff</p>
                  </div>
                  <table class="table" xml:id="mul-table1" frame="box">
                     <thead>
                        <tr valign="top">
                           <th>Alignment</th>
                           <th>CollateX</th>
                           <th>Tan Diff/Collate</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <th>Unison Alignments (all witnesses the same)</th>
                           <td>1198</td>
                           <td>515</td>
                        </tr>
                        <tr>
                           <th>All Alignments (unison and divergent)</th>
                           <td>2315</td>
                           <td>1932</td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               In general, the alignments were longer in <code class="code">tan:diff()</code> (as expected), and the tokenization and normalization outcomes appear so far to be
               reasonably reliable, though this is difficult to tell without being able to see the
               original passages in the TAN collation XML output. When this functionality is achieved,
               it would make sound sense to take a moment and adapt <code class="code">tan:collate()</code> to the pipeline of the <span class="ital">Frankenstein Variorum</span> project, since it appears even <q>out of the box</q> to produce fewer alignment problems. At the time of this writing, however, it is
               difficult to navigate the TAN library of included XSLT files and to fully understand
               how to intervene in the collation output process.</p>
            <p id="d2e677">Just because it is written in XSLT and is well-documented for public usability, does
               not mean an XSLT library is any more or less adaptable to intensive customization
               than a library made of other forms of code. As I write this concluding paragraph in
               mid-July 2022, my student assistant Yuying Jin and I have made a significant breakthrough
               in modifying our pre-processing Python script for collateX, and we have succeeded
               in handling the problem of spliced tokens, though we have now multipled the number
               of empty null-string tokens. Since it now appears easier to continue refining our
               Python script than to intervene in the labyrinthine XSLT library of functions, we
               will proceed in this direction for now, and look forward to future developments to
               improve the customization potential of the TAN library. The XSLT process promises
               to make the recursive pre-programming cycles in the Gothenburg model more amenable
               to clear documentation, and the effort to investigate two pre-processing methods via
               Python, and via XSLT, has proven fruitful to resolve many problems in our original
               process. The TAN library holds great promise for handling collations in XSLT, and
               while customizing its output proves for this moment no easy matter for an outsider
               to TAN, it seems a clear next step for this impressive XSLT library. 
               </p>
         </div>
         <div class="footnotes"><br><hr width="100" align="left">
            <div id="d2e98" class="footnote">
               <p><sup class="fn-label"><a href="#d2e98-ref" class="footnoteref">[1]</a></sup> Interedition Development Group, <q>The Gothenburg Model</q>, 2010-2019. <a href="https://collatex.net/doc/" class="link">https://collatex.net/doc/</a>. For more on the summit and workshop of collation software developers in 2009 that
                  formulated the Gothenburg model, see Ronald Haentjens Dekker, Dirk van Hulle, Gregor
                  Middell, Vincent Neyt, and Joris van Zundert. <q>Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital
                     Manuscript Project.</q> In <span class="ital">Digital Scholarship in the Humanities</span> 30:3 (December 2014) pp. 3-4.</p>
            </div>
            <div id="d2e123" class="footnote">
               <p><sup class="fn-label"><a href="#d2e123-ref" class="footnoteref">[2]</a></sup> Birnbaum, David J. and Elena Spadini, <q>Reassessing the locus of normalization in machine-assisted collation</q> <span class="ital">DHQ: Digital Humanities Quarterly</span> 14:3 (2020). <a href="http://www.digitalhumanities.org/dhq/vol/14/3/000489/000489.html" class="link">http://www.digitalhumanities.org/dhq/vol/14/3/000489/000489.html</a>.</p>
            </div>
            <div id="d2e140" class="footnote">
               <p><sup class="fn-label"><a href="#d2e140-ref" class="footnoteref">[3]</a></sup> John Bradley, <q>Text Tools</q> in <span class="ital">A Companion to Digital Humanities</span>, ed. Susan Schreibman, Ray Siemens, and John Unsworth (Wiley, 2004) pages 505-522.</p>
            </div>
            <div id="d2e176" class="footnote">
               <p><sup class="fn-label"><a href="#d2e176-ref" class="footnoteref">[4]</a></sup> Beshero-Bondar, Elisa Eileen. <q>Rebuilding a Digital Frankenstein by 2018: Reflections toward a Theory of Losses and
                     Gains in Up-Translation.</q> Presented at Up-Translation and Up-Transformation: Tasks, Challenges, and Solutions,
                  Washington, DC, July 31, 2017. In <span class="ital">Proceedings of Up-Translation and Up-Transformation: Tasks, Challenges, and Solutions.
                     Balisage Series on Markup Technologies</span>, vol. 20 (2017). <a href="https://doi.org/10.4242/BalisageVol20.Beshero-Bondar01." class="link">https://doi.org/10.4242/BalisageVol20.Beshero-Bondar01</a>.</p>
            </div>
            <div id="d2e190" class="footnote">
               <p><sup class="fn-label"><a href="#d2e190-ref" class="footnoteref">[5]</a></sup> See Beshero-Bondar, Elisa E., and Raffaele Viglianti. <q>Stand-off Bridges in the <span class="ital">Frankenstein Variorum</span> Project: Interchange and Interoperability within TEI Markup Ecosystems.</q> Presented at Balisage: The Markup Conference 2018, Washington, DC, July 31 - August
                  3, 2018. In <span class="ital">Proceedings of Balisage: The Markup Conference 2018. Balisage Series on Markup Technologies</span>, vol. 21 (2018); <a href="https://doi.org/10.4242/BalisageVol21.Beshero-Bondar01." class="link">https://doi.org/10.4242/BalisageVol21.Beshero-Bondar01</a>. I am also grateful to Michael Sperberg-McQueen and David Birnbaum for their guidance
                  of my efforts to <q>raise</q> flattened markup into hierarchical markup leading to a Balisage paper evaluating
                  multiple methods for attempting this. See Birnbaum, David J., Elisa E. Beshero-Bondar
                  and C. M. Sperberg-McQueen. <q>Flattening and unflattening XML markup: a Zen garden of XSLT and other tools.</q> Presented at Balisage: The Markup Conference 2018, Washington, DC, July 31 - August
                  3, 2018. In <span class="ital">Proceedings of Balisage: The Markup Conference 2018. Balisage Series on Markup Technologies</span>, vol. 21 (2018). <a href="https://doi.org/10.4242/BalisageVol21.Birnbaum01" class="link">https://doi.org/10.4242/BalisageVol21.Birnbaum01</a>. 
                  </p>
            </div>
            <div id="d2e264" class="footnote">
               <p><sup class="fn-label"><a href="#d2e264-ref" class="footnoteref">[6]</a></sup> On Trojan markup, see Steve DeRose, <q>Markup Overlap: A Review and a Horse</q>, <span class="ital">Extreme Markup Languages</span> 2004. <a href="https://www.academia.edu/42096176/Markup_Overlap_A_Review_and_a_Horse" class="link">https://www.academia.edu/42096176/Markup_Overlap_A_Review_and_a_Horse</a>.
                  and Sperberg-McQueen, C. M. <q>Representing concurrent document structures using Trojan Horse markup.</q> Presented at Balisage: The Markup Conference 2018, Washington, DC, July 31 - August
                  3, 2018. In <span class="ital">Proceedings of Balisage: The Markup Conference 2018. Balisage Series on Markup Technologies</span>, vol. 21 (2018). <a href="https://doi.org/10.4242/BalisageVol21.Sperberg-McQueen01" class="link">https://doi.org/10.4242/BalisageVol21.Sperberg-McQueen01</a>.</p>
            </div>
            <div id="d2e337" class="footnote">
               <p><sup class="fn-label"><a href="#d2e337-ref" class="footnoteref">[7]</a></sup> On the parallel segmentation method for handling a critical apparatus in TEI P5 see
                  the TEI Guidelines: TEI Consortium, eds. <q>4.3.2 Floating Texts.</q> <span class="ital">TEI P5: Guidelines for Electronic Text Encoding and Interchange</span>. 4.4.0. April 19, 2022. TEI Consortium. http://www.tei-c.org/release/doc/tei-p5-doc/en/html/DS.html#DSFLT
                  (2022-07-19).</p>
            </div>
            <div id="d2e429" class="footnote">
               <p><sup class="fn-label"><a href="#d2e429-ref" class="footnoteref">[8]</a></sup> Our production pipeline unfolds in a series of GitHub repos at <a href="https://github.com/FrankensteinVariorum" class="link">https://github.com/FrankensteinVariorum</a>. The repo storing and documenting the post-collation XSLT files, not the focus of
                  discussion for this paper but likely of interest  to our readers is <a href="https://github.com/FrankensteinVariorum/fv-postCollation" class="link">https://github.com/FrankensteinVariorum/fv-postCollation</a>.</p>
            </div>
            <div id="d2e595" class="footnote">
               <p><sup class="fn-label"><a href="#d2e595-ref" class="footnoteref">[9]</a></sup> Kalvesmaki, Joel. <q>String Comparison in XSLT with tan:diff().</q> Presented at Balisage: The Markup Conference 2021, Washington, DC, August 2 - 6,
                  2021. In <span class="ital">Proceedings of Balisage: The Markup Conference 2021. Balisage Series on Markup Technologies</span>, vol. 26 (2021). <a href="https://doi.org/10.4242/BalisageVol26.Kalvesmaki01" class="link">https://doi.org/10.4242/BalisageVol26.Kalvesmaki01</a>.</p>
            </div>
         </div>
      </div>
      <div id="balisage-footer">
         <h3><!--* balisage-html.xsl 519.38 *--><i>Balisage:</i>&nbsp;<small>The Markup Conference</small></h3>
      </div>
   </body>
</html>