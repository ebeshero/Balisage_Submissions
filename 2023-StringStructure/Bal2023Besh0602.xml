<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="balisage-1-5.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<?xml-model href="balisage-1-5.rng" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<?xml-stylesheet type="text/xsl" href="balisage-proceedings-html.xsl"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink"
    version="5.0-subset Balisage-1.5">

    <title>Markup modeling and migratory workflows in the context of AI and big data analytics</title>
    <subtitle>Reflections on the data modeling groundwork of the digital humanities</subtitle>
    <info>
        <abstract><!--ebb: Many thanks for reading this in a preliminary, inchoate form. 
            What's here is certainly too long for an abstract and too short for a paper, 
            but I hope it provides a good idea of how the paper will develop! 
              -->
            <para>This paper emerges from intensive immersion in the migration of text formats, from so-called <quote>unstructured</quote>, <quote>plain</quote> or <quote>semi-structured</quote> text to the schema-driven construction of document trees with the XML stack. The author organizes research and university course projects that purposefully engage in such migration, which is more than just a technical reformatting or mapping of structures. The work also involves an uneasy navigation between distinct cultures of computationally-assisted text analysis—cultures or communities of practice that occasionally overlap but often appear to be at distinct odds with one another. In the digital humanities that serves as an academic <quote>home</quote> to the author, these cultures reflect the practice of natural language processing and big text data analytics, usually facilitated by Python or R libraries, and the practice of markup and transformation with the XML stack.</para>
            <para>The author’s course project assignemnts for a <quote>Large-Scale Text Analysis</quote> course require accessing large language models including spaCy, NLTK, or most recently, openAI's Generative Pre-trained Transformer (GPT) 3.5 and 4. These projects also purposefully map text data into XML document structures derived or built from the source text files. This facilitates work with XQuery that may also <quote>roundtrip</quote> to output data formats in TSV or JSON for use with visualization software that supports analysis and bug detection. The author’s related research involves comparing digital editions representing different stages of a novel. As discussed at previous Balisage conferences, the author reads XML document structures as strings in order to include markup tags in the comparison as meaningful indicators of alteration (see <link xlink:href="https://www.balisage.net/Proceedings/vol27/html/Beshero-Bondar01/BalisageVol27-Beshero-Bondar01.html">Adventures in Correcting XML Collation Problems with Python and XSLT</link>).</para>
            <para>From the author’s professional and informal interactions with colleagues and peers in the digital humanities, questions arise about the significance or even the need for markup in the <quote>distant reading</quote> varieties of text analysis. In these professional circles, coding is usually written in the context of statistical data packages. The ability to study patterns in language over thousands of texts and form conclusions aided by trained natural language processing models is highly valued in the area of digital humanities involving computational text analysis. However, it is  sometimes highly controversial and embarrassing when statistical significance is questioned. Yet the author’s engagement through the TEI community with digital scholarly editions explores another area of document data modeling with markup ontologies and schemas, where declarative markup raises questions of sharability. In these circles, occupied by members of the Text Encoding Initiative, we are haunted by questions of whether multiple projects can model document data in the same way following an international community’s shared guidelines for encoding electronic texts, or whether our guidelines are too permissive of variety to be standardized and generalizable and useful for analytics.</para>
            <para>There are problems in the isolation of these communities, and opportunities to be seen in drawing them together. In this paper, I will reflect on the adventures of teaching and roundtripping​ between text data formats this semester—some insights into strings, data containers, markup—and the control vs. chaos we face in <quote>pulling strings</quote>. Pressed in January 2023 with a question or provocation about whether markup is relevant in the era of artificial intelligence driven my large language models, I propose a detailed response in this paper about its many uses, even at scale, in a very messy world of digital ephemera.</para>
               <para> In this paper I will discuss the importance of markup in sustaining of electronic documents from pre-millenial archives, in preparing these for analytical research. Sections to be developed include:
                
                <orderedlist>
                    <listitem>
                        <para>Cultures of digital text research: big data analytics vs. digital curation</para>
                    </listitem>
                    <listitem>
                        <para></para>
                    </listitem>
                    <listitem>
                        <para>String cleaning: Markup as a destination in remediating early electronic texts</para>
                    </listitem>
                    
                
                    <listitem>
                        <para>Not <quote>either or</quote> but <quote>both and</quote>: Decanting text data between container elements and data frames</para>
                    </listitem>
                    
                    <listitem>
                        <para>Declarative conclusions or futurities?</para>
                    </listitem>
                    
                </orderedlist>
                
         
In these sections, I will prioritize the often unrecognized middle space of transformation and the intellectual work required to migrate texts from one format to another. My unusual experience of moving between communities has introduced me to a certain differential pattern, in which I pull selected strings from XML to work with in the processing libraries du jour and supply the XML with data that I might well wish to remove or change when those processing libraries improve. Further, if I want to share those documents in a readable way, XML serves as a basis for many different ways to visualize the documents. The markup facilitates the preservation of metadata, annotations, the basis of scholarly editing for the long range. If digital humanists of our moment are too impatient for this work and prioritze the grand statements made at distant scale and the details matter little, the same archive can share a choice of strings to pull.</para>
                
            <para>The movement between formats can thus represent a stage in the life-cycle of the digital documents. When addressing that movement, markup provides us an orderly determination of structures that might easily be lost, thrown away in data cleaning or blurred in a process of reading by new software. In my lab and my classes, we work with regular expression patterns to mark up collections of documents with Python for the identification of structures from patterns of spaces and newlines, the discovery of section boundaries, act and scene divisions, the separation of data from metadata. Applying markup in this way is a method of descriptive document analysis, and it assists in creating what we might think of in laboratory terms as labled bins or trays. When we do the query work of pulling for string-matches in container elements and translate that data for work in other formats, the oft-cited critique of markup as binding or confining documents to a single hierarchy <!--(c.f. Johanna Drucker)--> seem overstated. The apparent flatness of a text file belies its not-so-hidden hierarchies, and the combinations of string and ngrams does not mean its data structures are nonexistent or unimportant. Nor does it mean that the unmarked, half-expressed, or abbreviated metadata about the author's circumstances or the software for composition is irrelevant—even at scale. These are the terms of electronic paleography and digital provenience in our time.</para>
            <para>In a moment of eager excitement, confusion, and fear about the potential disruptive influences of generative language models, I want to address the reliability, precision, and broad applicability of markup technologies in a well-equipped text lab for expanding research questions and for supplying a counterbalance to the anxiety-ridden, often speculative work that digital humanists promote as <quote>distant reading</quote>. When the statistical bases of a model are subject to rapid change with the next update, and when developers of generative language models conceal their sources for commercial reasons, we would do well to inspect our tools and curated collections for brittle dependencies. Declarative markup can offer a modicum of control for humaniites scholarship, perhaps a means to directly address the ephemerality of unstable technology stacks.</para>
            <para>
                
                <!--(to reference: 
                * Mordechai Levy-Eichel and Daniel Scheinerman, "Digital humanists need to learn how to count: A prominent recent book in the field suffers serious methodological pitfalls." The Chronicle of Higher Education 17 May 2022. https://www.chronicle.com/article/digital-humanists-need-to-learn-how-to-count 
                
                * essays from The Shape of Data in Digital Humanities, ed. Julia Flanders and Fotis Jannidis. London: Routledge, 2018. 
                      * (note: Intro, contribution on the TEI by Lou Burnard, conclusion by Sperburg-McQueen). 
                
                * Michael Sperburg McQueen, "What does descriptive markup contribute to digital humanities?". Digital Humanities Concepts 2015 Conference presentation (slides + Geoffrey Rockwell's notes). https://philosophi.ca/pmwiki.php/Main/DigitalHumanitiesConcepts2015 
               
                
                * Ted Underwood, The Stone and the shell blog posts:  "Seven ways humanists are using computers to understand text" 4 June 2015:
                https://tedunderwood.com/2015/06/04/seven-ways-humanists-are-using-computers-to-understand-text/ and "Emerging conversations between literary history and sociology." 02 December 2015: https://tedunderwood.com/2015/12/02/emerging-conversations-between-literary-history-and-sociology/
                
                * Gregory J. Palermo, "Transforming Text: Four Valences of a Digital Humanities Informed Writing Analytics" Journal of Writing Analytics 
                    Vol. 1 (2017). DOI: 10.37514/JWA-J.2017.1.1.11. https://wac.colostate.edu/docs/jwa/vol1/palermo.pdf
     
                
                * author's (professional and friendly) January 2023 conversation about ChatGPT and the supposed demise/irrelevance of the TEI w/ Underwood and digital humanities colleagues on Mastodon wherein limited assumptions about markup's representational capacities were exposed: in Underwood's Mastodon thread: https://sigmoid.social/@TedUnderwood/109730986869388754 
           -->
            </para>
    
        </abstract>
        <author>
            <personname>
                <firstname>Elisa</firstname>
                <othername>E.</othername>
                <surname>Beshero-Bondar</surname>
            </personname>
            <personblurb>
                <para>Elisa Beshero-Bondar explores and teaches document data modeling with the XML
                    family of languages. She serves on the TEI Technical Council and is the founder
                    and organizer of the <link xlink:href="https://digitalmitford.org"
                        xlink:type="simple" xlink:show="new" xlink:actuate="onRequest">Digital
                        Mitford project</link> and <link
                        xlink:href="https://digitalmitford.github.io/DigMitCS/" xlink:type="simple"
                        xlink:show="new" xlink:actuate="onRequest">its usually annual coding
                        school</link>. She experiments with visualizing data from complex document
                    structures like epic poems and with computer-assisted collation of differently
                    encoded editions of <link xlink:href="https://frankensteinvariorum.github.io/"
                        xlink:type="simple" xlink:show="new" xlink:actuate="onRequest"><emphasis
                            role="ital">Frankenstein</emphasis></link>. Her ongoing adventures with
                    markup technologies are documented on <link xlink:href="https://newtfire.org"
                        xlink:type="simple" xlink:show="new" xlink:actuate="onRequest">her
                        development site at newtfire.org</link>.</para>
            </personblurb>
            <affiliation>
                <jobtitle>Chair</jobtitle>
                <orgname>TEI Technical Council</orgname>
            </affiliation>
            <affiliation>
                <jobtitle>Professor of Digital Humanities</jobtitle>
                <jobtitle>Program Chair of Digital Media, Arts, and Technology</jobtitle>
                <orgname>Penn State Erie, The Behrend College</orgname>
            </affiliation>
            <email>eeb4@psu.edu</email>
        </author>
        <keywordset role="author">
        
            <keyword>Python</keyword>
            <keyword>XSLT</keyword>
            
        </keywordset>
    </info>
 
    
</article>
